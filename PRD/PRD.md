Below is a ready-to-paste Markdown PRD for your PetriBench open-source testing suite. It follows a classic structure (Executive Summary → Requirements → Road-map) so teammates can skim or dive deep. Feel free to tweak sections or ask for a canvas if you’d like to iterate collaboratively.

# Product Requirements Document (PRD)
## PetriBench – Minimal, Repeatable Docker Environments for Memory-Benchmarking Runtimes

| **Version** | 0.9 (Draft) |
|-------------|-------------|
| **Author**  | Keng Susumpow + Contributors |
| **Last Updated** | 13 Jul 2025 |
| **Status** | Draft for review |

---

## 1  Executive Summary
### 1.1  Purpose
PetriBench provides a **single, reproducible test harness** to measure per-language memory cost (RSS / PSS / USS) and startup time under strictly identical conditions. It ships ultra-thin Docker images—one per language—layered on a shared `debian:bookworm-slim` base.

### 1.2  Problem Statement
Ad-hoc benchmarks mix distros (`alpine`, `ubuntu`, custom AMIs) and measurement tools (`time -v`, `/usr/bin/top`), producing noisy, non-comparable numbers. Developers cannot tell whether overhead comes from the language runtime, distribution, or instrumentation.

### 1.3  Solution Overview
PetriBench offers:

| Layer | What it Delivers |
|-------|------------------|
| **Base OS** | Hardened `debian:bookworm-slim` plus `procps`, `smem2`, minimal shell |
| **Language Add-ons** | Overlay layers that add only the compiler/runtime needed (GCC, Node, CPython, OpenJDK, .NET, etc.) |
| **Harness CLI** | `petri bench <lang> <program>` wraps `docker run`, captures RSS/PSS/USS, emits JSON/CSV |
| **CI Matrix** | GitHub Actions workflow to re-benchmark on every PR and publish a Markdown report |

---

## 2  Goals & Non-Goals
### 2.1  Goals
1. **Reproducible Baseline**: same kernel, libc, and measurement tooling for all languages.
2. **Lean Footprint**: base image ≤ 40 MB compressed; per-language layer increments ≤ 200 MB.
3. **First-class Metrics**: one-command capture of RSS, PSS, USS, wall-clock start-up, and peak RSS.
4. **Easy Extensibility**: add a new language with a single `Dockerfile.<lang>` and manifest entry.
5. **CI Friendly**: works in GitHub Actions runners (x86_64 & arm64).

### 2.2  Non-Goals
* Not a general-purpose production image set (security patches only for benchmarking stability).
* CPU profiling and energy benchmarking are out-of-scope for v1.
* No orchestration beyond single-container tests (multi-node scenarios deferred).

---

## 3  Personas & Use-Cases
| Persona | Scenario | Success Metric |
|---------|----------|----------------|
| **Language Engineer** | Compare Go, Rust, and C baseline memory on ARM. | <code>petri bench rust empty.rs</code> returns within 5 sec and exports CSV. |
| **DevOps Lead** | Validate .NET vs Java footprint before picking runtime for FaaS. | Report shows ΔUSS ≤ 12 MB between “hello” and FizzBuzz. |
| **Academic Researcher** | Publish paper on container cold-start. | PetriBench provides DOI-tagged image digests for citation. |

---

## 4  Functional Requirements
| ID | Requirement | Priority |
|----|-------------|----------|
| **F-1** | Provide `bench-base` image with glibc, `/proc` tools, non-root user. | Must |
| **F-2** | Offer official language layers: C/C++, Rust, Go, Python, Node, Java, .NET. | Must |
| **F-3** | Support `petri bench` CLI to run N iterations and stream metrics. | Must |
| **F-4** | Emit outputs in **JSON**, **CSV**, and pretty table for Slack. | Should |
| **F-5** | Allow custom Dockerfile overlays via `petri init <lang>` template. | Should |
| **F-6** | Publish daily GitHub Action that rebuilds & pushes images with CVE fixes. | Should |
| **F-7** | Provide a Markdown report generator (`petri report latest.json`). | Could |

---

## 5  Non-Functional Requirements
* **Deterministic Builds** – pinned Debian snapshot + language version in `versions.lock`.
* **Cross-Arch** – images built for `linux/amd64` and `linux/arm64`.
* **Security** – no root in containers; base hardened with `dpkg --purge --auto-remove apt-lists`.
* **Performance** – instrumentation overhead < 2 ms CPU and < 8 KB memory.
* **Licensing** – project under Apache-2.0; third-party packages comply.

---

## 6  Technical Architecture
```text
┌─────────────────────┐
│ GitHub Actions (CI) │
└─────────┬───────────┘
          v
┌───────────────────────────┐
│ Docker Buildx Matrix      │
│ ├ bench-base (Debian)     │
│ ├ bench-python            │
│ ├ bench-go                │
│ └ …                       │
└─────────┬─────────────────┘
          v
┌───────────────────────────┐
│ petri CLI (Python)        │
│ • docker run --rm         │
│ • parse /proc/*/smaps     │
│ • JSON/CSV output         │
└─────────┬─────────────────┘
          v
┌───────────────────────────┐
│ Slack / Markdown reporter │
└───────────────────────────┘

6.1  Base Image (bench-base)
	•	debian:bookworm-slim
	•	Installed: procps, curl, python3, pip install smem2
	•	Non-root user tester, working dir /workspace

6.2  Language Layer Example (Python)

ARG PY_VER=3.12
FROM bench-base
RUN apt-get update && \
    apt-get install -y --no-install-recommends python${PY_VER} && \
    apt-get clean
CMD ["python3", "-m", "timeit", "-s", "pass"]

6.3  Measurement Flow
	1.	docker run the target program.
	2.	Sleep configurable warm-up (default 200 ms).
	3.	Read /proc/$PID/smaps_rollup → PSS/USS.
	4.	Parse /proc/$PID/status → VmRSS.
	5.	Kill process; repeat N times for mean/σ.

⸻

7  API / CLI Sketch

# One-off run
petri bench python examples/fizzbuzz.py --runs 50 --output fizz.json

# Generate Slack-ready table
petri report fizz.json --format slack

# Add new language layer interactively
petri init zig --zig-version 0.12.0


⸻

8  Metrics & KPIs (v1)

KPI	Target
Cold-start reproducibility (std dev/mean)	< 5 % across 100 runs
Base image compressed size	≤ 40 MB
Additional size per language	≤ 200 MB
CI pipeline duration	≤ 10 min for full matrix
Time to add new language	≤ 30 min with template


⸻

9  Road-map & Milestones

Date	Milestone	Deliverables
2025-07-31	M0 Bootstrap	Repo scaffolding, bench-base build passing
2025-08-15	M1 Core Languages	C/C++, Rust, Go, Python layers + CLI prototype
2025-08-30	M2 Managed Runtimes	Java, dotnet layers; JSON/CSV exporter
2025-09-15	M3 Report Gen & Slack Bot	petri report, Slack formatting, GH Pages site
2025-10-01	M4 Community Release	v1.0 tag, blog post, OpenSSF badge


⸻

10  Risks & Mitigations

Risk	Impact	Likelihood	Mitigation
Musl vs glibc incompat (Alpine forks)	Builds fail	Medium	Stay glibc for baseline; musl variant flagged experimental
Kernel features missing on CI runners	Measurement inaccurate	Low	Use Ubuntu 24.04 LTS runners (Linux 6.8+) with smaps_rollup
Language images grow over time	KPI fail	Medium	CI alerts when layer > target; weekly prune audit
Contributor environment mismatch	Onboarding friction	Low	Provide justfile + DevContainer for VS Code

⸻

11  Open Questions
	1.	Do we need a GUI dashboard (Grafana-lite) in v1 or leave it CLI-only?
	2.	Should we support Windows containers for .NET comparison?
	3.	Embed energy measurement hooks (RAPL, Perf) or defer to v2?

⸻

12  Glossary
	•	RSS – Resident-Set Size, all pages resident in RAM.
	•	PSS – Proportional Set Size, shared pages divided by #processes.
	•	USS – Unique Set Size, private pages only.
	•	smem2 – Modern fork of smem that reads /proc/*/smaps_rollup.
	•	glue layer – Language-specific Docker stage sitting atop bench-base.

⸻

13  References
	•	Linux /proc documentation – https://docs.kernel.org/filesystems/proc.html#maps-smaps
	•	smem2 project – https://github.com/thermionics/smem
	•	Debian Slim guidelines – https://wiki.debian.org/Cloud/DebianDockerImage
	•	GitHub Container Registry – https://docs.github.com/en/packages/working-with-a-github-packages-registry

---

**Next steps**

1. Share in Slack for feedback (`/code` fence preserves formatting).
2. Decide on M0 scope—base image & CLI skeleton.
3. Ping me if you’d like a DevContainer, GitHub Actions YAML, or an initial README scaffold.